(window.webpackJsonp=window.webpackJsonp||[]).push([[27],{332:function(e,t,a){"use strict";a.r(t);var n=a(24),s=Object(n.a)({},function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("p",[a("img",{attrs:{src:"https://www.microshare.io/wp-content/uploads/2018/10/KSQL-Logo-New.png",alt:"An image"}})]),e._v(" "),a("h1",{attrs:{id:"ksql"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ksql","aria-hidden":"true"}},[e._v("#")]),e._v(" KSQL")]),e._v(" "),a("p",[e._v("KSQL is the streaming SQL engine for Apache Kafka®. It provides an easy-to-use yet powerful interactive SQL interface for stream processing on Kafka, without the need to write code in a programming language.")]),e._v(" "),a("p",[e._v("KSQL is built on Kafka Streams, a robust stream processing framework that is part of Apache Kafka.")]),e._v(" "),a("p",[e._v("KSQL is not ANSI SQL compliant, for now there are no defined standards on streaming SQL languages")]),e._v(" "),a("ul",[a("li",[a("strong",[e._v("KSQL Server")]),e._v(": The KSQL server runs the engine that executes KSQL queries. This includes processing, reading, and writing data to and from the target Kafka cluster")]),e._v(" "),a("li",[a("strong",[e._v("KSQL CLI")]),e._v(":  You can interactively write KSQL queries by using the KSQL command line interface (CLI). The KSQL CLI acts as a client to the KSQL server.")])]),e._v(" "),a("p",[e._v("KSQL servers, clients, queries, and applications run outside of Kafka brokers, in separate JVM instances, or in separate clusters entirely.")]),e._v(" "),a("ul",[a("li",[a("p",[a("strong",[e._v("KSQL Engine")]),e._v("\nThe KSQL engine executes KSQL statements and queries. You define your application logic by writing KSQL statements, and the engine builds and runs the application on available KSQL servers. Each KSQL server instance runs a KSQL engine. Under the hood, the engine parses your KSQL statements and builds corresponding Kafka Streams topologies.")])]),e._v(" "),a("li",[a("p",[a("strong",[e._v("REST Interface")]),e._v("\nThe REST server interface enables communicating with the KSQL engine from the CLI, Confluent Control Center, or from any other REST client.")])])]),e._v(" "),a("p",[a("strong",[e._v("Data Definition Language (DDL) Statements")]),e._v("\nImperative verbs that define metadata on the KSQL server by adding, changing, or deleting streams and tables. Data Definition Language statements modify metadata only and don't operate on data. You can use these statements with declarative DML statements.")]),e._v(" "),a("p",[e._v("The DDL statements include:")]),e._v(" "),a("ul",[a("li",[e._v("CREATE STREAM")]),e._v(" "),a("li",[e._v("CREATE TABLE")]),e._v(" "),a("li",[e._v("DROP STREAM")]),e._v(" "),a("li",[e._v("DROP TABLE")]),e._v(" "),a("li",[e._v("CREATE STREAM AS SELECT (CSAS)")]),e._v(" "),a("li",[e._v("CREATE TABLE AS SELECT (CTAS)")])]),e._v(" "),a("p",[a("strong",[e._v("Data Manipulation Language (DML) Statements")]),e._v("\nDeclarative verbs that read and modify data in KSQL streams and tables. Data Manipulation Language statements modify data only and don't change metadata. The KSQL engine compiles DML statements into Kafka Streams applications, which run on a Kafka cluster like any other Kafka Streams application.")]),e._v(" "),a("p",[e._v("The DML statements include:")]),e._v(" "),a("ul",[a("li",[e._v("SELECT")]),e._v(" "),a("li",[e._v("INSERT INTO")]),e._v(" "),a("li",[e._v("CREATE STREAM AS SELECT (CSAS)")]),e._v(" "),a("li",[e._v("CREATE TABLE AS SELECT (CTAS)")])]),e._v(" "),a("h1",{attrs:{id:"deployment"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#deployment","aria-hidden":"true"}},[e._v("#")]),e._v(" Deployment")]),e._v(" "),a("ul",[a("li",[a("p",[a("strong",[e._v("Interactive")]),e._v(" – data exploration and pipeline development. KSQL shares statements with servers in the cluster over the command topic. The "),a("strong",[e._v("command topic")]),e._v(" stores every KSQL statement, along with some metadata that ensures the statements are built compatibly across KSQL restarts and upgrades.")])]),e._v(" "),a("li",[a("p",[a("strong",[e._v("Headless")]),e._v(" – long-running production environments. The REST interface isn't available, so you assign workloads to KSQL servers by using a SQL file. The SQL file contains the KSQL statements and queries that define your application. KSQL stores metadata in an internal topic called the "),a("strong",[e._v("config topic")]),e._v(".")])])]),e._v(" "),a("p",[e._v("KSQL enables distributing the processing load for your KSQL applications across all KSQL Server instances, and you can add more KSQL Server instances without restarting your applications.")]),e._v(" "),a("p",[e._v("Join KSQL engines to the same service pool by using the "),a("strong",[e._v("ksql.service.id")]),e._v(" property.")]),e._v(" "),a("h1",{attrs:{id:"ksql-query-lifecycle"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ksql-query-lifecycle","aria-hidden":"true"}},[e._v("#")]),e._v(" KSQL Query Lifecycle")]),e._v(" "),a("ul",[a("li",[e._v("Register a KSQL stream or table from an existing Kafka topic with a DDL statement, like "),a("strong",[e._v("CREATE STREAM "),a("my-stream",[e._v(" WITH "),a("topic-name")],1)],1),e._v(".")]),e._v(" "),a("li",[e._v("Express your app by using a KSQL statement, like "),a("strong",[e._v("CREATE TABLE AS SELECT FROM "),a("my-stream")],1),e._v(".")]),e._v(" "),a("li",[e._v("KSQL parses your statement into an abstract syntax tree (AST).")]),e._v(" "),a("li",[e._v("KSQL uses the AST and creates the logical plan for your statement.")]),e._v(" "),a("li",[e._v("KSQL uses the logical plan and creates the physical plan for your statement.")]),e._v(" "),a("li",[e._v("KSQL generates and runs the Kafka Streams application.")]),e._v(" "),a("li",[e._v("You manage the application as a STREAM or TABLE with its corresponding persistent query.")])]),e._v(" "),a("h1",{attrs:{id:"ksql-vs-kstreams"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#ksql-vs-kstreams","aria-hidden":"true"}},[e._v("#")]),e._v(" KSQL vs KStreams")]),e._v(" "),a("table",[a("thead",[a("tr",[a("th"),e._v(" "),a("th",[e._v("KSQL")]),e._v(" "),a("th",[e._v("KStreams")])])]),e._v(" "),a("tbody",[a("tr",[a("td",[e._v("You write:")]),e._v(" "),a("td",[e._v("KSQL statements")]),e._v(" "),a("td",[e._v("JVM applications")])]),e._v(" "),a("tr",[a("td",[e._v("Graphical UI")]),e._v(" "),a("td",[e._v("Yes, in Confluent Control Center")]),e._v(" "),a("td",[e._v("No")])]),e._v(" "),a("tr",[a("td",[e._v("Console")]),e._v(" "),a("td",[e._v("Yes")]),e._v(" "),a("td",[e._v("No")])]),e._v(" "),a("tr",[a("td",[e._v("Data formats")]),e._v(" "),a("td",[e._v("Avro, JSON, CSV")]),e._v(" "),a("td",[e._v("Any data format, including Avro, JSON, CSV, Protobuf, XML")])]),e._v(" "),a("tr",[a("td",[e._v("REST API included")]),e._v(" "),a("td",[e._v("Yes")]),e._v(" "),a("td",[e._v("No, but you can implement your own")])]),e._v(" "),a("tr",[a("td",[e._v("Runtime included")]),e._v(" "),a("td",[e._v("Yes, the KSQL server")]),e._v(" "),a("td",[e._v("Applications run as standard JVM processes")])]),e._v(" "),a("tr",[a("td",[e._v("Queryable state")]),e._v(" "),a("td",[e._v("No")]),e._v(" "),a("td",[e._v("Yes")])])])]),e._v(" "),a("p",[e._v("Usually, KSQL isn't a good fit for BI reports, ad-hoc querying, or queries with random access patterns, because it's a continuous query system on data streams.")]),e._v(" "),a("h1",{attrs:{id:"time-and-windows"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#time-and-windows","aria-hidden":"true"}},[e._v("#")]),e._v(" Time and Windows")]),e._v(" "),a("p",[e._v("In KSQL, a record is an immutable representation of an event in time. Each record carries a "),a("strong",[e._v("timestamp")]),e._v(". Timestamps are used by time-dependent operations, like aggregations and joins.")]),e._v(" "),a("ul",[a("li",[a("strong",[e._v("Event-time")]),e._v(": The time when a record is created by the data source.")]),e._v(" "),a("li",[a("strong",[e._v("Ingestion-time")]),e._v(": The time when a record is stored in a topic partition by a Kafka broker.")]),e._v(" "),a("li",[a("strong",[e._v("Processing-time")]),e._v(": The time when the record is consumed by a stream processing application.")])]),e._v(" "),a("p",[e._v("Don't mix streams or tables that have different time semantics.")]),e._v(" "),a("p",[e._v("Topic -> "),a("strong",[e._v("message.timestamp.type")])]),e._v(" "),a("ul",[a("li",[a("em",[e._v("CreateTime")]),e._v(" - Event-time")]),e._v(" "),a("li",[a("em",[e._v("LogAppendTime")]),e._v(" - Ingestion-time")])]),e._v(" "),a("p",[e._v("By default, when KSQL imports a topic to create a stream, it uses the record's timestamp, but you can add the WITH(TIMESTAMP='some-field') clause to use a different field from the record's value as the timestamp")]),e._v(" "),a("p",[a("strong",[e._v("Output Streams")])]),e._v(" "),a("ul",[a("li",[e._v("When new output records are generated by processing an input record directly, output record timestamps are inherited from input record timestamps.")]),e._v(" "),a("li",[e._v("When new output records are generated by a periodic function, the output record timestamp is defined as the current internal time of the stream task.")]),e._v(" "),a("li",[e._v("For aggregations, the timestamp of the resulting update record is taken from the latest input record that triggered the update.")])]),e._v(" "),a("h2",{attrs:{id:"window-types"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#window-types","aria-hidden":"true"}},[e._v("#")]),e._v(" Window types")]),e._v(" "),a("ul",[a("li",[e._v("Hopping Window:\tTime-based & Fixed-duration, overlapping windows")]),e._v(" "),a("li",[e._v("Tumbling Window\tTime-based & Fixed-duration, non-overlapping, gap-less windows")]),e._v(" "),a("li",[e._v("Session Window:\tSession-based\t& Dynamically-sized, non-overlapping, data-driven windows")])]),e._v(" "),a("p",[e._v("KSQL supports using windows in JOIN queries by using the "),a("strong",[e._v("WITHIN")]),e._v(" clause")]),e._v(" "),a("p",[e._v("SHOW STREAMS and EXPLAIN "),a("query",[e._v(" statements run against the KSQL server that the KSQL client is connected to. They don’t communicate directly with Kafka. CREATE STREAM WITH "),a("topic",[e._v(" and CREATE TABLE WITH "),a("topic",[e._v(" write metadata to the KSQL command topic. Persistent queries based on CREATE STREAM AS SELECT and CREATE TABLE AS SELECT read and write to Kafka topics. Non-persistent queries based on SELECT that are stateless only read from Kafka topics, for example SELECT … FROM foo WHERE …. Non-persistent queries that are stateful read and write to Kafka, for example, COUNT and JOIN. The data in Kafka is deleted automatically when you terminate the query with CTRL-C.")])],1)],1)],1)])},[],!1,null,null,null);t.default=s.exports}}]);