<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Apache Kafka notes | From Sources</title>
    <meta name="description" content="Some Apache Kafka &amp; Confluent notes.">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,300,400,500,700,900|Material+Icons">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css">
    
    <link rel="preload" href="/fromsources-web/assets/css/0.styles.08ac24d0.css" as="style"><link rel="preload" href="/fromsources-web/assets/js/app.02d0b93b.js" as="script"><link rel="preload" href="/fromsources-web/assets/js/4.39b54307.js" as="script"><link rel="preload" href="/fromsources-web/assets/js/1.a380d8b6.js" as="script"><link rel="preload" href="/fromsources-web/assets/js/26.599b0752.js" as="script"><link rel="prefetch" href="/fromsources-web/assets/js/10.83b89ba7.js"><link rel="prefetch" href="/fromsources-web/assets/js/11.d56c6a5c.js"><link rel="prefetch" href="/fromsources-web/assets/js/12.bbee6052.js"><link rel="prefetch" href="/fromsources-web/assets/js/13.33ef5605.js"><link rel="prefetch" href="/fromsources-web/assets/js/14.d88a16f6.js"><link rel="prefetch" href="/fromsources-web/assets/js/15.c8f6c376.js"><link rel="prefetch" href="/fromsources-web/assets/js/16.6dbab5ad.js"><link rel="prefetch" href="/fromsources-web/assets/js/17.eb8352a3.js"><link rel="prefetch" href="/fromsources-web/assets/js/18.41214848.js"><link rel="prefetch" href="/fromsources-web/assets/js/19.80f3f8f2.js"><link rel="prefetch" href="/fromsources-web/assets/js/20.483d8f6c.js"><link rel="prefetch" href="/fromsources-web/assets/js/21.a5d7ae49.js"><link rel="prefetch" href="/fromsources-web/assets/js/22.68a8635c.js"><link rel="prefetch" href="/fromsources-web/assets/js/23.f69f02d9.js"><link rel="prefetch" href="/fromsources-web/assets/js/24.640c54c0.js"><link rel="prefetch" href="/fromsources-web/assets/js/25.c91b56e6.js"><link rel="prefetch" href="/fromsources-web/assets/js/27.fe568593.js"><link rel="prefetch" href="/fromsources-web/assets/js/28.7a0b8288.js"><link rel="prefetch" href="/fromsources-web/assets/js/29.9bec897a.js"><link rel="prefetch" href="/fromsources-web/assets/js/30.05b488a8.js"><link rel="prefetch" href="/fromsources-web/assets/js/5.82763f16.js"><link rel="prefetch" href="/fromsources-web/assets/js/6.12c846f7.js"><link rel="prefetch" href="/fromsources-web/assets/js/7.3024d01a.js"><link rel="prefetch" href="/fromsources-web/assets/js/8.4da819e1.js"><link rel="prefetch" href="/fromsources-web/assets/js/9.1b941426.js"><link rel="prefetch" href="/fromsources-web/assets/js/vendors~docsearch.f92eab53.js">
    <link rel="stylesheet" href="/fromsources-web/assets/css/0.styles.08ac24d0.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/fromsources-web/" class="home-link router-link-active"><!----> <span class="site-name">From Sources</span></a> <div class="links" style="max-width:nullpx;"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/fromsources-web/" class="nav-link">Home</a></div><div class="nav-item"><a href="/fromsources-web/blog/" class="nav-link router-link-active">Blog</a></div><div class="nav-item"><a href="/fromsources-web/about/" class="nav-link">About</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/fromsources-web/" class="nav-link">Home</a></div><div class="nav-item"><a href="/fromsources-web/blog/" class="nav-link router-link-active">Blog</a></div><div class="nav-item"><a href="/fromsources-web/about/" class="nav-link">About</a></div> <!----></nav>  <!----> </div> <div class="page"> <div class="content default"><p><img src="https://kafka.apache.org/22/images/kafka-apis.png" alt="An image"></p> <h1 id="publish-subscribe"><a href="#publish-subscribe" aria-hidden="true" class="header-anchor">#</a> Publish/subscribe</h1> <p>Publish/subscribe messaging is a pattern that is characterized by the sender (publisher) of a piece of data (message) not specifically directing it to a receiver. Instead, the publisher classifies the message somehow, and that receiver (subscriber) subscribes to receive certain classes of messages. Pub/sub systems often have a broker, a central point where messages are published, to facilitate this.</p> <h1 id="apache-kafka"><a href="#apache-kafka" aria-hidden="true" class="header-anchor">#</a> Apache Kafka</h1> <p>Apache Kafka is a publish/subscribe messaging system. It is often described as a “distributed commit log” or more recently as a “distributing streaming platform.”
Data within Kafka is stored durably, in order, and can be read deterministically. In addition, the data can be distributed within the system to provide additional protections against failures, as well as significant opportunities for scaling performance.</p> <p>Goals:</p> <ul><li>Decouple producers and consumers by using a push-pull model</li> <li>Provide persistence for message data within the messaging system to allow multiple consumers</li> <li>Optimize for high throughput of messages</li> <li>Allow for horizontal scaling of the system to grow as the data streams grew.</li></ul> <p>The unit of data within Kafka is called a <strong>message</strong> =&gt; array of bytes
<strong>KEY</strong> (optional) =&gt; Message metadata =&gt; byte array
<strong>Value</strong> =&gt; Message =&gt; byte array</p> <p><strong>Batches</strong>: Messages are written into Kafka in batches. Reduces overhead of individual roundtrips acroos the network, for each meessage.
Batches are typically compressed =&gt; more efficient data transfer.</p> <h1 id="brokers-and-clusters"><a href="#brokers-and-clusters" aria-hidden="true" class="header-anchor">#</a> Brokers and Clusters</h1> <ul><li><p>Broker =&gt; Kafka server. Operate as part of a <em>cluster</em>.</p></li> <li><p>Broker receives messages from producers =&gt; assigns offsets to them =&gt; commits the messages to storage on disk.</p></li> <li><p>Broker serve consumers =&gt; respond to fetch requests for partitions and responding with the messages.</p></li></ul> <h2 id="configuration"><a href="#configuration" aria-hidden="true" class="header-anchor">#</a> Configuration</h2> <ul><li><p>At the broker level, you control the <strong>default.replication.factor</strong> for automatically created topics.
A replication factor of N allows you to lose N-1 brokers while still being able to read and write data to the topic reliably. Higher replication factor leads to higher availability, higher reliability, and fewer disasters</p></li> <li><p><strong>unclean.leader.election.enable</strong> -- default true. If we allow out-of-sync replicas to become leaders, we risk data loss and data inconsistencies. If we set it to false, we choose to wait for the original leader to come back online, resulting in lower availability</p></li> <li><p><strong>min.insync.replicas</strong> to 2, then you can only write to a partition in the topic if at least two out of the three replicas are in-sync -- Read-only mode.
<strong>min.insync.replicas</strong> only matters if <strong>acks=all</strong></p></li></ul> <h2 id="controller"><a href="#controller" aria-hidden="true" class="header-anchor">#</a> Controller</h2> <p>The controller is one of the Kafka brokers that, in addition to the usual broker functionality, is responsible for electing partition leaders. The first broker that starts in the cluster
becomes the controller by creating an ephemeral node in ZooKeeper called <strong>/controller</strong>.
Kafka uses Zookeeper’s ephemeral node feature to elect a controller and to notify the controller when nodes join and leave the cluster. The controller is responsible for electing leaders among the partitions and replicas whenever it notices nodes join and leave the cluster. The controller uses the epoch number to prevent a “split brain” scenario where two nodes believe each is the current controller.</p> <p>Controller =&gt; one broker will also function as the cluster <em>controller</em> - There is only one controller in a cluster at all times.
Admin operaitons:</p> <ul><li>Assigning partitions to brokers</li> <li>Monitoring for broker failures</li></ul> <p>The <em>leader of the partition</em> =&gt; partition is owned by a single broker in the cluster. A partition may be assigned to multiple brokers, which will result in the partition being replicated. Another broker can take over leadership if there is a broker failure. However, all consumers and producers operating on that partition must connect to the leader.</p> <h1 id="topics"><a href="#topics" aria-hidden="true" class="header-anchor">#</a> Topics</h1> <p>Messages in Kafka are categorized into <strong>topics</strong>. Topics are additionally broken down into a number of partitions
A topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic, just within a single partition.</p> <p>Replicas are spread across available brokers, and each replica = one broker. RF 3 = 3 brokers</p> <p>Dynamic topic configurations are maintained in Zookeeper.</p> <p><strong>Partitions</strong> are also the way that Kafka provides redundancy and scalability. Each partition can be hosted on a different server, which means that a single topic can be scaled horizontally across multiple servers to provide performance far beyond the ability of a single server.</p> <h1 id="producers"><a href="#producers" aria-hidden="true" class="header-anchor">#</a> Producers</h1> <p><strong>Producers</strong> create new messages (publishers/writers). A message will be produced to a specific topic.</p> <p>The producer does not care what partition a specific message is written to and will balance messages over all partitions of a topic evenly.
In some cases, the producer will direct messages to specific partitions. This is typically done using the message key and a partitioner that will generate a hash of the key and map it to a specific partition. This assures that all messages produced with a given key will get written to the same partition</p> <p>Keys are necessary if you require strong ordering or grouping for messages that share the same key. If you require that messages with the same key are always seen in the correct order, attaching a key to messages will ensure messages with the same key always go to the same partition in a topic. Kafka guarantees order within a partition, but not across partitions in a topic, so alternatively not providing a key - which will result in round-robin distribution across partitions - will not maintain such order.</p> <p><strong>ProducerRecord</strong>, which must include the topic we want to send the record to and a value.
Optionally, we can also specify a <em>key</em> and/or a <em>partition</em>. Once we send the <em>ProducerRecord</em>, the first thing the producer will do is serialize the key and value objects to <em>ByteArrays</em> so they can be sent over the network. Next, the data is sent to a <strong>partitioner</strong>. If we specified a partition in the <em>ProducerRecord</em>, the partitioner doesn’t do anything and simply returns the partition we specified. If we didn’t, the <em>partitioner</em> will choose a partition for us, usually based on the <em>ProducerRecord key</em>. Once a partition is selected, the producer knows which topic and partition the record will go to. It then adds the record to a batch of records that will also be sent to the same topic and partition. A separate thread is responsible for sending those batches of records to the appropriate Kafka brokers.</p> <p>When the broker receives the messages, it sends back a response. If the messages were successfully written to Kafka, it will return a <strong>RecordMetadata</strong> object with the topic, partition, and the offset of the record within the partition. If the broker failed to write the messages, it will return an error. When the producer receives an error, it
may retry sending the message a few more times before giving up and returning an error.</p> <p><strong>Mandatory properties</strong></p> <ul><li><strong>bootstrap.servers</strong>: List of host:port pairs of brokers.</li> <li><strong>key.serializer</strong>: Name of a class that will be used to serialize the keys. Setting key.serializer is required even if you intend to send only values.</li> <li><strong>value.serializer</strong>: Name of a class that will be used to serialize the values</li></ul> <p><strong>Methods</strong></p> <ul><li><strong>Fire-and-forget</strong>: We send a message to the server and don’t really care if it arrives succesfully or not. Most of the time, it will arrive successfully, since Kafka is highly available and the producer will retry sending messages automatically. However, some messages will get lost using this method.</li> <li><strong>Synchronous</strong> send. We send a message, the send() method returns a Future object, and we use get() to wait on the future and see if the send() was successful or not.</li> <li><strong>Asynchronous</strong> send: We call the send() method with a callback function, which gets triggered when it receives a response from the Kafka broker.</li></ul> <p>Errors before sending the message to Kafka:</p> <ul><li><strong>SerializationException</strong> when it fails to serialize the message.</li> <li><strong>BufferExhaustedException</strong> or <strong>TimeoutException</strong> if the buffer is full</li> <li><strong>InterruptException</strong> if the sending thread was interrupted.</li></ul> <p>KafkaProducer type of errors.<br> <strong>Retriable</strong> errors are those that can be resolved by sending the message again. For example, &quot;Connection error&quot;.
Some errors will not be resolved by retrying. For example, “message size too large.”</p> <h2 id="prdoducer-configuration"><a href="#prdoducer-configuration" aria-hidden="true" class="header-anchor">#</a> Prdoducer configuration</h2> <ul><li><p><strong>acks</strong> controls how many partition replicas must receive the record before the producer can consider the write successful.</p> <ul><li><strong>acks=0</strong>: producer will not wait for a reply from the broker before assuming the message was sent successfully -- very high throughput</li> <li><strong>acks=1</strong>: producer will receive a success response from the broker the moment the leader replica received the message</li> <li><strong>acks=all</strong>: producer will receive a success response from the broker once all in-sync replicas received the message. -- safest mode</li></ul></li> <li><p><strong>buffer.memory</strong> : This sets the amount of memory the producer will use to buffer messages waiting to be sent to brokers.
<strong>block.on.buffer.full</strong> parameter (replaced with <strong>max.block.ms</strong> in release 0.9.0.0, which allows blocking for a certain time and then throwing an exception).</p></li> <li><p><strong>compression.type</strong>: By default, messages are sent uncompressed. This parameter can be set to <em>snappy</em>, <em>gzip</em>, or <em>lz4</em>.</p></li> <li><p><strong>retries</strong>: The value of the retries parameter will control how many times the producer will retry sending the message before giving up and notifying the client of an issue. By default, the producer will wait 100ms between retries, but you can control this using the <strong>retry.backoff.ms</strong> parameter.</p></li> <li><p><strong>batch.size</strong> (in bytes!) controls how many bytes of data to collect before sending messages to the Kafka broker. Set this as high as possible, without exceeding available memory. Enabling compression can also help make more compact batches and increase the throughput of your producer.</p></li> <li><p><strong>linger.ms</strong> forces the producer to wait to send messages, hence increasing the chance of creating batches</p></li> <li><p><strong>client.id</strong> This can be any string, and will be used by the brokers to identify messages sent from the client.</p></li> <li><p><strong>max.in.flight.requests.per.connection</strong> how many messages the producer will send to the server without receiving responses. Setting this to 1 will guarantee that messages will be written to the broker in the order in which they were sent.</p></li> <li><p><strong>request.timeout.ms</strong>: how long the producer will wait for a reply from the server when sending data</p></li> <li><p><strong>metadata.fetch.timeout.ms</strong>: how long the producer will wait for a reply from the server when requesting metadata</p></li> <li><p><strong>timeout.ms</strong>: controls the time the broker will wait for in-sync replicas to acknowledge the message in order to meet the acks configuration—the broker will return an error if the time elapses without the necessary acknowledgments.</p></li> <li><p><strong>max.block.ms</strong>: how long the producer will block when calling <em>send()</em> and when explicitly requesting metadata via <em>partitionsFor()</em>.</p></li> <li><p><strong>max.request.size</strong>: This setting controls the size of a produce request sent by the producer.
the broker has its own limit on the size of the largest message it will accept (<strong>message.max.bytes</strong>). It is usually a good idea to have these configurations match, so the producer will not attempt to send messages of a size that will be rejected by the broker.</p></li> <li><p><strong>receive.buffer.bytes</strong>: size of the TCP send buffer</p></li> <li><p><strong>send.buffer.bytes</strong>: size of the TCP receive buffer</p></li></ul> <p>If these are set to -1, the OS defaults will be used.</p> <p><strong>Ordering Guarantees</strong>
If guaranteeing order is critical, we recommend setting in.flight.requests.per.session=1 to make sure that while a batch of messages is retrying, additional messages will not
be sent.This will severely limit the throughput of the producer, so only use this when order is important.</p> <h1 id="consumers"><a href="#consumers" aria-hidden="true" class="header-anchor">#</a> Consumers</h1> <p><strong>Consumers</strong> read messages. (subscribers/readers). The consumer subscribes to one or more topics and reads the messages in the order in which they were produced. The consumer keeps track of which messages it has already consumed by keeping track of the <em>offset</em> of messages.
Kafka does not track acknowledgments from consumers the way many JMS queues do. Instead, it allows consumers to use Kafka to track their position (offset) in each partition.</p> <p><strong>commit</strong>: Action of updating the current position in the partition. Consumer produces a message to Kafka, to a special <strong>__consumer_offsets</strong> topic, with the committed offset for each partition.</p> <p><strong>offset</strong>: integer  -- Each message in a given partition has a unique offset.</p> <p><strong>Consumer group</strong> =&gt; consumers that work together to consume a topic. When multiple consumers are subscribed to a topic and belong to the same consumer group, each consumer in
the group will receive messages from a different subset of the partitions in the topic.
Each partition is only consumed by one member. The mapping of a consumer to a partition is often called <strong>ownership of the partition</strong> by the consumer.
If we add more consumers to a single group with a single topic than we have partitions,some of the consumers will be idle and get no messages at all.
Multiple topics can be passed as a list or regex pattern.
Kafka transfers data with zero-copy and sends the raw bytes it receives from the producer straight to the consumer, leveraging the RAM available as page cache.</p> <p>Moving partition ownership from one consumer to another is called a <strong>rebalance</strong>.
Rebalance is basically a short window of unavailability of the entire consumer group. Ehen partitions are moved from one consumer to another, the consumer loses its current state; if it was caching any data, it will need to refresh its caches—slowing down the application until the consumer sets up its state again.
The way consumers maintain membership in a consumer group and ownership of the partitions assigned to them is by sending heartbeats to a Kafka broker designated as the <strong>group coordinator</strong></p> <ul><li><p>If the committed offset is smaller than the offset of the last message the client processed, the messages between the last processed offset and the committed offset will
be <strong>processed twice</strong></p></li> <li><p>If the committed offset is larger than the offset of the last message the client actually processed, all messages between the last processed offset and the committed offset
<strong>will be missed</strong> by the consumer group</p></li> <li><p>There are many different ways to implement <strong>exactly-once</strong> semantics by storing offsets and data in an external store, but all of them will need to use the
<strong>ConsumerRebalanceListener</strong> and <strong>seek()</strong> to make sure offsets are stored in time and that the consumer starts reading messages from the correct location.</p></li></ul> <p>Closing the consumer will commit offsets if needed and will send the group coordinator a message that the consumer is leaving the group. The consumer coordinator will trigger rebalancing immediately and you won’t need to wait for the session to time out before partitions from the consumer you are closing will be assigned to another consumer in the group.</p> <p><strong>assign()</strong> can be used for manual assignment of a partition to a consumer, in which case <strong>subscribe() must not be used</strong>. Assign() takes TopicPartition object as an argument</p> <p><strong>auto.offset.reset=latest</strong></p> <p><strong>at-most once consuming</strong> scenario. Which offset commit strategy would you recommend? commit the offsets right after receiving a batch from a call to .poll().
Before processing the data.</p> <p>Consumer offsets are stored in a Kafka topic __consumer_offsets</p> <p>In case the consumer has the wrong leader of a partition, it will issue a metadata request. The Metadata request can be handled by any node, so clients know afterwards which broker are the designated leader for the topic partitions. Produce and consume requests can only be sent to the node hosting partition leader.</p> <ul><li>Always commit offsets after events were processed</li> <li>Commit frequency is a trade-off between performance and number of duplicates in the event of a crash</li> <li>Make sure you know exactly what offsets you are committing</li> <li>Rebalances</li> <li>Consumers may need to retry</li> <li>Consumers may need to maintain state</li> <li>Handling long processing times</li> <li>Exactly-once delivery
<ul><li>idempotent writes</li></ul></li></ul> <p><strong>Mandatory properties</strong></p> <ul><li><p><strong>bootstrap.servers</strong>: List of host:port pairs of brokers.</p></li> <li><p><strong>key.deserializer</strong>: classes that can take a byte array and turn it into a Java object.</p></li> <li><p><strong>value.deserializer</strong>: classes that can take a byte array and turn it into a Java object</p></li> <li><p><strong>group.id</strong> : the consumer group the KafkaConsumer instance belongs to (Optional but always used)</p></li></ul> <p><strong>subcribe()</strong> :</p> <ul><li>List of topic names</li> <li>Regular expression</li></ul> <p>One consumer per thread is the rule.</p> <h2 id="consumer-configuration"><a href="#consumer-configuration" aria-hidden="true" class="header-anchor">#</a> Consumer configuration</h2> <ul><li><p><strong>fetch.min.bytes</strong>: minimum amount of data that the consumer wants to receive from the broker when fetching records.</p></li> <li><p><strong>fetch.max.wait.ms</strong>: control how long to wait. Default 500ms</p></li> <li><p><strong>max.partition.fetch.bytes</strong> : This property controls the maximum number of bytes the server will return per partition. Default 1MB.
max.partition.fetch.bytes must be larger than the largest message a broker will accept</p></li> <li><p><strong>session.timeout.ms</strong>: The amount of time a consumer can be out of contact with the brokers while still considered alive -- defaults to 3 seconds
<strong>heatbeat.interval.ms</strong> must be lower than session.timeout.ms</p></li> <li><p><strong>auto.offset.reset</strong> Default is <strong>latest</strong>. <strong>earliest</strong> to start consuming from beginning. For KSQL, SET 'auto.offset.reset'='earliest';</p></li> <li><p><strong>enable.auto.commit</strong> Default is true
<strong>auto.commit.interval.ms</strong> how frequently offsets will be committed. Default 5seg.</p> <p>AutoCommit=true, avoid duplicates is hard: Call to poll will always commit the last offset returned by the previous poll. It is critical to always process all the events returned by poll() before calling poll() again.</p> <p>AutoCommit=false, <strong>commitSync()</strong>. This API will commit the latest offset returned by poll() and return once the offset is committed, throwing an exception if commit fails for some reason. One drawback of manual commit is that the application is blocked until the broker responds to the commit request.
Throughput can be improved by committing less frequently, but then we are increasing the number of potential duplicates that a rebalance will create.
The drawback is that while commitSync() will retry the commit until it either succeeds or encounters a nonretriable failure, <strong>commitAsync() will not retry</strong>.</p></li> <li><p><strong>partition.assignment.strategy</strong> <strong>Range</strong>: Assigns to each consumer a consecutive subset of partitions from each topic it subscribes to.
<strong>RoundRobin</strong>: Takes all the partitions from all subscribed topics and assigns them to consumers sequentially, one by one.</p></li> <li><p><strong>client.id</strong>: This can be any string, and will be used by the brokers to identify messages sent from the client.</p></li> <li><p><strong>max.poll.records</strong>: This controls the maximum number of records that a single call to <em>poll()</em> will return.</p></li> <li><p><strong>receive.buffer.bytes</strong>: size of the TCP buffer</p></li> <li><p><strong>send.buffer.bytes</strong>: sizes of the TCP buffer</p></li></ul> <h1 id="retention"><a href="#retention" aria-hidden="true" class="header-anchor">#</a> Retention</h1> <p>The durable storage of messages for some period of time. Kafka brokers are configured with a default retention setting for topics, either retaining messages for some period of time or until the topic reaches a certain size in bytes. Once these limits are reached, messages are expired and deleted so that the retention configuration is a minimum amount of data available at any time.
Individual topics can also be configured with their own retention settings. Topics can also be configured as <em>log compacted</em> =&gt; Kafka will retain only the last message produced with a specific key. This can be useful for changelog-type data, whereonly the last update is interesting.</p> <h1 id="multiple-clusters"><a href="#multiple-clusters" aria-hidden="true" class="header-anchor">#</a> Multiple Clusters</h1> <ul><li>Segregation of types of data</li> <li>Isolation for security requirements</li> <li>Multiple datacenters (disaster recovery)</li></ul> <p>The replication mechanisms within the Kafka lusters are designed only to work within a single cluster, not between multiple clusters.</p> <p><strong>MirrorMaker</strong> =&gt; Kafka consumer and producer, linked together with a
queue. Messages are consumed from one Kafka cluster and produced for another.</p> <h1 id="multiple-producers"><a href="#multiple-producers" aria-hidden="true" class="header-anchor">#</a> Multiple Producers</h1> <h1 id="multiple-consumers"><a href="#multiple-consumers" aria-hidden="true" class="header-anchor">#</a> Multiple Consumers</h1> <h1 id="disk-based-retention"><a href="#disk-based-retention" aria-hidden="true" class="header-anchor">#</a> Disk-Based Retention</h1> <p>Consumers do not always need to work in real time. Durable retention
means that if a consumer falls behind, either due to slow processing or a burst in traffic, there is no danger of losing data.</p> <h1 id="scalable"><a href="#scalable" aria-hidden="true" class="header-anchor">#</a> Scalable</h1> <p>Start with a single broker =&gt; expand to a larger cluster.
Expansions can be performed while the cluster is online, with no impact.</p> <p><strong>High Performance</strong>
Apache Kafka carries messages between the various members of the infrastructure, providing a consistent interface for all clients</p> <h1 id="use-cases"><a href="#use-cases" aria-hidden="true" class="header-anchor">#</a> Use Cases</h1> <ul><li>Activity tracking</li> <li>Messaging</li> <li>Metrics and logging</li> <li>Commit log</li> <li>Stream processing</li></ul> <h1 id="zookeeper"><a href="#zookeeper" aria-hidden="true" class="header-anchor">#</a> Zookeeper</h1> <p>Different Kafka components subscribe to the <strong>/brokers/ids</strong> path in Zookeeper where brokers are registered so they get notified when brokers are added or removed.</p> <p><strong>ACLs</strong> are stored in Zookeeper node <strong>/kafka-acls/</strong> by default.</p> <p>2181 - client port, 2888 - peer port, 3888 - leader port</p> <p>Kafka components that are watching the list of brokers will be notified that the broker is gone.Even though the node representing the broker is gone when the broker is stopped,
the broker ID still exists in other data structures. For example, the list of replicas of each topic (see “Replication” on page 97) contains the broker IDs for the replica. This
way, if you completely lose a broker and start a brand new broker with the ID of the old one, it will immediately join the cluster in place of the missing broker with the
same partitions and topics assigned to it.</p> <p>Dynamic topic configurations are stored in Zookeeper.</p> <h1 id="replication"><a href="#replication" aria-hidden="true" class="header-anchor">#</a> Replication</h1> <p>Replication is critical because it is the way Kafka guarantees availability and durability when individual nodes inevitably fail.
Each topic is partitioned, and each partition can have multiple replicas. Those replicas are stored on brokers, and each broker typically stores hundreds or even thousands of replicas belonging to different topics and partitions.</p> <ul><li><strong>Leader replica</strong>: Each partition has a single replica designated as the leader. All produce and consume requests go through the leader, in order to guarantee consistency.</li> <li><strong>Follower replica</strong>: All replicas for a partition that are not leaders are called followers.Followers don’t serve client requests; their only job is to replicate messages from the leader
and stay up-to-date with the most recent messages the leader has.</li></ul> <p>The amount of time a follower can be inactive or behind before it is considered out of sync is controlled by the <strong>replica.lag.time.max.ms</strong> configuration parameter.</p> <p>Kafka is configured with <strong>auto.leader.rebalance.enable=true</strong>, which will check if the preferred leader replica is not the current leader but is in-sync and trigger leader election
to make the preferred leader the current leader.</p> <h1 id="request-processing"><a href="#request-processing" aria-hidden="true" class="header-anchor">#</a> Request processing</h1> <p>All requests sent to the broker from a specific client will be processed in the order in which they were received</p> <p><strong>Headers</strong></p> <ul><li><strong>Request type</strong>: also called API key</li> <li><strong>Request version</strong>: so the brokers can handle clients of different versions and respond accordingly</li> <li><strong>Correlation ID</strong>: a number that uniquely identifies the request and also appears in the response and in the error logs</li> <li><strong>Client ID</strong>: used to identify the application that sent the request</li></ul> <p>The network threads are responsible for taking requests from client connections, placing them in a request queue, and picking up responses from a response queue and sending them back to clients.</p> <p>Both produce requests and fetch requests have to be sent to the leader replica of a partition</p> <h2 id="metadata-requests"><a href="#metadata-requests" aria-hidden="true" class="header-anchor">#</a> Metadata requests</h2> <p>Kafka clients use another request type called a <strong>metadata</strong> request, which includes a list of topics the client is interested in. The server response specifies which partitions exist in the topics, the replicas for each partition, and which replica is the leader. Metadata requests can be sent to any broker because all brokers have a metadata cache that contains this information.
Refresh intervals <strong>metadata.max.age.ms</strong> configuration parameter</p> <h2 id="producer-requests"><a href="#producer-requests" aria-hidden="true" class="header-anchor">#</a> Producer requests</h2> <ul><li>If acks is set to 0 or 1, the broker will respond immediately.</li> <li>If acks is set to all, the request will be stored in a buffer called <strong>purgatory</strong> until the leader observes that the follower replicas replicated the message, at which point a response is sent to the client.</li></ul> <h2 id="fetch-requests"><a href="#fetch-requests" aria-hidden="true" class="header-anchor">#</a> Fetch Requests</h2> <ul><li>Clients also specify a limit to how much data the broker can return for each partition.</li> <li>If the offset exists, the broker will read messages from the partition, up to the limit set by the client in the request, and send the messages to the client. Kafka famously uses a <strong>zero-copy</strong> method to send the messages to the clients—this means that Kafka sends messages from the file directly to the network channel without any intermediate buffers. &gt;&gt; performance.</li> <li>Clients can also set a lower boundary on the amount of data returned, and define a timeout.
“If you didn’t satisfy the minimum amount of data to send within x milliseconds, just send what you got.”</li> <li><strong>replica.lag.time.max.ms</strong>—the amount of time a replica can be delayed in replicating new messages while still being considered in-sync.</li></ul> <h1 id="physical-storage"><a href="#physical-storage" aria-hidden="true" class="header-anchor">#</a> Physical storage</h1> <p><strong>log.dirs</strong>: list of directories in which the partitions will be stored</p> <h2 id="partition-allocation"><a href="#partition-allocation" aria-hidden="true" class="header-anchor">#</a> Partition allocation</h2> <ul><li>Spread replicas evenly among brokers</li> <li>For each partition, each replica is on a different broker</li> <li>If the brokers have rack information, then assign the replicas for each partition to different racks if possible</li></ul> <h2 id="file-management"><a href="#file-management" aria-hidden="true" class="header-anchor">#</a> File Management</h2> <ul><li>Configures a retention period for each topic</li> <li>Split each partition into <strong>segments</strong>. As a Kafka broker is writing to a partition, if the segment limit is reached, we close the file and start a new one. The segment we are currently writing to is called an <strong>active segment</strong>. The active segment is never deleted</li></ul> <h2 id="file-format"><a href="#file-format" aria-hidden="true" class="header-anchor">#</a> File format</h2> <ul><li>Each segment is stored in a single data file
<ul><li>Kafka messages and their offsets</li></ul></li> <li>This means that if you are using compression on the producer, sending larger batches means better compression both over the network and on the broker disks.</li></ul> <p>Kafka brokers ship with the <strong>DumpLogSegment</strong> tool, which allows you to look at a partition segment in the filesystem and examine its contents.</p> <div class="language- extra-class"><pre class="language-text"><code> bin/kafka-run-class.sh kafka.tools.DumpLogSegments
</code></pre></div><h2 id="indexes"><a href="#indexes" aria-hidden="true" class="header-anchor">#</a> Indexes</h2> <p>Kafka maintains an index for each partition. The index maps offsets to segment files and positions within the file.
Indexes are also broken into segments, so we can delete old index entries when the messages are purged.</p> <h2 id="compaction"><a href="#compaction" aria-hidden="true" class="header-anchor">#</a> Compaction</h2> <p>Retention policy on a topic:</p> <ul><li><strong>delete</strong>, which deletes events older than retention time</li> <li><strong>compact</strong>, which only stores the most recent value for each key in the topic. If the topic contains null keys, compaction will fail. <strong>log.cleaner.enabled</strong></li></ul> <p>The compact policy never compacts the current segment. Messages are eligble for compaction only on inactive segments.</p> <h1 id="guarantee"><a href="#guarantee" aria-hidden="true" class="header-anchor">#</a> Guarantee</h1> <ul><li>Kafka provides order guarantee of messages in a partition.</li> <li>Produced messages are considered “committed” when they were written to the partition on all its in-sync replicas. Producers can choose to receive acknowledgments of sent messages when the message was fully committed, when it was written to the leader, or when it was sent over the network.</li> <li>Messages that are committed will not be lost as long as at least one replica remains alive</li> <li>Consumers can only read messages that are committed.</li></ul> <p>Having a message written in multiple replicas is how Kafka provides durability of messages in the event of a crash.</p> <h1 id="settings"><a href="#settings" aria-hidden="true" class="header-anchor">#</a> Settings</h1> <p>Setting <strong>unclean.leader.election.enable</strong> to <strong>true</strong> means we allow out-of-sync replicas to become leaders, we will lose messages when this occurs, effectively losing credit card payments and making our customers very angry.</p></div> <div class="page-edit"><!----> <!----></div> <!----> <section><div class="layout my-5 column wrap align-center"><div class="flex my-3 xs12 sm4"><div class="text-xs-center"><div><span data-link="#share-facebook"><i aria-hidden="true" class="v-icon fab fa-facebook-square theme--light blue--text text--darken-2" style="font-size:36px;cursor:pointer;"></i></span> <span data-link="#share-linkedin"><i aria-hidden="true" class="v-icon fab fa-linkedin theme--light blue--text text--darken-2" style="font-size:36px;cursor:pointer;"></i></span> <span data-link="#share-twitter"><i aria-hidden="true" class="v-icon fab fa-twitter-square theme--light blue--text text--darken-2" style="font-size:36px;cursor:pointer;"></i></span></div></div></div></div></section> </div> <!----></div><div class="global-ui"><!----></div></div>
    <script src="/fromsources-web/assets/js/app.02d0b93b.js" defer></script><script src="/fromsources-web/assets/js/4.39b54307.js" defer></script><script src="/fromsources-web/assets/js/1.a380d8b6.js" defer></script><script src="/fromsources-web/assets/js/26.599b0752.js" defer></script>
  </body>
</html>
